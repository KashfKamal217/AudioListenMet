{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdab00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479a7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sr):\n",
    "    \"\"\"Apply random augmentation\"\"\"\n",
    "    if np.random.rand() < 0.3:  # Add noise\n",
    "        audio = audio + 0.005 * np.random.randn(len(audio))\n",
    "    if np.random.rand() < 0.3:  # Pitch shift\n",
    "        audio = librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=np.random.choice([-2, 2]))\n",
    "    if np.random.rand() < 0.3:  # Time stretch\n",
    "        rate = np.random.uniform(0.8, 1.2)\n",
    "        audio = librosa.effects.time_stretch(y=audio, rate=rate)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414da377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Audio Loading & Augmentation ----------------------\n",
    "def load_audio(file_path, sr=16000, augment=False):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    if augment:\n",
    "        audio = augment_audio(audio, sr)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bcb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Feature Extraction ----------------------\n",
    "def preprocess(audio, sr=16000):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    return mfccs.T[:80]\n",
    "\n",
    "def pad_sequence(mfccs, max_length=80):\n",
    "    return np.pad(mfccs, ((0, max(0, max_length - len(mfccs))), (0, 0)), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32987c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Pair Creation ----------------------\n",
    "def create_word_pairs(data_dir, max_pairs_per_class=100):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    all_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    for word_dir in all_dirs:\n",
    "        word_path = os.path.join(data_dir, word_dir)\n",
    "        audio_files = [os.path.join(word_path, f) for f in os.listdir(word_path) if f.endswith('.m4a') or f.endswith('.mp3')]\n",
    "\n",
    "        # Positive pairs\n",
    "        for i in range(len(audio_files)):\n",
    "            for j in range(i + 1, len(audio_files)):\n",
    "                pairs.append((audio_files[i], audio_files[j]))\n",
    "                labels.append(1)\n",
    "                if len(pairs) >= max_pairs_per_class:\n",
    "                    break\n",
    "\n",
    "        # Negative pairs\n",
    "        for other_dir in all_dirs:\n",
    "            if other_dir != word_dir:\n",
    "                other_path = os.path.join(data_dir, other_dir)\n",
    "                other_files = [os.path.join(other_path, f) for f in os.listdir(other_path) if f.endswith('.m4a') or f.endswith('.mp3')]\n",
    "                if audio_files and other_files:\n",
    "                    pairs.append((audio_files[0], other_files[0]))\n",
    "                    labels.append(0)\n",
    "\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acbb92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pairs(pairs, augment=False):\n",
    "    X1, X2 = [], []\n",
    "    for file1, file2 in pairs:\n",
    "        a1 = preprocess(load_audio(file1, augment=augment))\n",
    "        a2 = preprocess(load_audio(file2, augment=augment))\n",
    "        X1.append(pad_sequence(a1))\n",
    "        X2.append(pad_sequence(a2))\n",
    "    return np.array(X1), np.array(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75dc054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all files to: dataurdu1\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(zip_path, extract_to='.'):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(f\"Extracted all files to: {extract_to}\")\n",
    "\n",
    "# Example usage\n",
    "zip_path = r'C:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\urdualphabets_voices.zip'\n",
    "extract_to = 'dataurdu1'  \n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "unzip_file(zip_path, extract_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377f5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Encoder for feature extraction\n",
    "def build_encoder(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, 5, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Custom Layer for Absolute Difference\n",
    "class AbsDifference(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.abs(inputs[0] - inputs[1])\n",
    "\n",
    "# Siamese Network\n",
    "def build_siamese(input_shape):\n",
    "    encoder = build_encoder(input_shape)\n",
    "\n",
    "    input1 = tf.keras.Input(shape=input_shape)\n",
    "    input2 = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    encoded1 = encoder(input1)\n",
    "    encoded2 = encoder(input2)\n",
    "\n",
    "    diff = AbsDifference()([encoded1, encoded2])  # No Lambda issues\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(diff)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input1, input2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a984b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KASHF KAMAL\\AppData\\Local\\Temp\\ipykernel_16624\\3914915262.py:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file_path, sr=sr)\n",
      "c:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "c:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">61,824</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ abs_difference      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AbsDifference</span>)     │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ abs_difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m13\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m13\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m61,824\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ abs_difference      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mAbsDifference\u001b[0m)     │                   │            │ sequential[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ abs_difference[\u001b[38;5;34m0\u001b[0m… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,953</span> (242.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,953\u001b[0m (242.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,953</span> (242.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,953\u001b[0m (242.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------- Training ----------------------\n",
    "SEED = 42\n",
    "data_dir = r\"C:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\dataurdu1\\alphabets\\alphabets\"\n",
    "\n",
    "pairs, labels = create_word_pairs(data_dir)\n",
    "X1, X2 = process_pairs(pairs, augment=True)\n",
    "y = np.array(labels)\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# input_shape = X1_train.shape[1:]\n",
    "# model = build_siamese(input_shape)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "input_shape = (X1_train.shape[1], X1_train.shape[2])  # Corrected input shape\n",
    "model = build_siamese(input_shape)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f2e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b9686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.8262 - loss: 0.5090 - val_accuracy: 0.8905 - val_loss: 0.2557 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8979 - loss: 0.2627 - val_accuracy: 0.8994 - val_loss: 0.2144 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9223 - loss: 0.2182 - val_accuracy: 0.9408 - val_loss: 0.1843 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9320 - loss: 0.1946 - val_accuracy: 0.9408 - val_loss: 0.2059 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9408 - loss: 0.1706 - val_accuracy: 0.9586 - val_loss: 0.1438 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9297 - loss: 0.1830 - val_accuracy: 0.9527 - val_loss: 0.1357 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9460 - loss: 0.1554 - val_accuracy: 0.9615 - val_loss: 0.1227 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9578 - loss: 0.1402 - val_accuracy: 0.9645 - val_loss: 0.1130 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9630 - loss: 0.1247 - val_accuracy: 0.9675 - val_loss: 0.1046 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9645 - loss: 0.1212 - val_accuracy: 0.9586 - val_loss: 0.1070 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9675 - loss: 0.1092 - val_accuracy: 0.9615 - val_loss: 0.0944 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9682 - loss: 0.1013 - val_accuracy: 0.9615 - val_loss: 0.0945 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9749 - loss: 0.1016 - val_accuracy: 0.9467 - val_loss: 0.1208 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9593 - loss: 0.1201 - val_accuracy: 0.9615 - val_loss: 0.0954 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9756 - loss: 0.0944 - val_accuracy: 0.9645 - val_loss: 0.0991 - learning_rate: 5.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9749 - loss: 0.0841 - val_accuracy: 0.9763 - val_loss: 0.0833 - learning_rate: 5.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9778 - loss: 0.0770 - val_accuracy: 0.9763 - val_loss: 0.0941 - learning_rate: 5.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9815 - loss: 0.0792 - val_accuracy: 0.9734 - val_loss: 0.0909 - learning_rate: 5.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9800 - loss: 0.0755 - val_accuracy: 0.9793 - val_loss: 0.0783 - learning_rate: 5.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0704 - val_accuracy: 0.9763 - val_loss: 0.0772 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    [X1_train, X2_train], y_train,\n",
    "    validation_data=([X1_test, X2_test], y_test),\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('siamese_model_optimized2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db2daf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def predict_similarity2(file1, file2, model):\n",
    "    # Load and preprocess first audio\n",
    "    audio1, sr1 = librosa.load(file1, sr=16000)\n",
    "    a1 = preprocess(audio1, sr=sr1)\n",
    "    a1 = pad_sequence(a1)\n",
    "    \n",
    "    # Load and preprocess second audio\n",
    "    audio2, sr2 = librosa.load(file2, sr=16000)\n",
    "    a2 = preprocess(audio2, sr=sr2)\n",
    "    a2 = pad_sequence(a2)\n",
    "\n",
    "    # Add batch dimension\n",
    "    a1 = np.expand_dims(a1, axis=0)\n",
    "    a2 = np.expand_dims(a2, axis=0)\n",
    "\n",
    "    # Predict similarity\n",
    "    pred = model.predict([a1, a2])[0][0]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e3f3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Similarity Score: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KASHF KAMAL\\AppData\\Local\\Temp\\ipykernel_25900\\3243090477.py:11: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio2, sr2 = librosa.load(file2, sr=16000)\n",
      "c:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "score = predict_similarity2(r\"C:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\dataurdu\\urdualphabets11\\bay\\bay_1.mp3\",r\"C:\\Users\\KASHF KAMAL\\Documents\\AudioListenMet\\dataurdu\\urdualphabets11\\bay\\bay_10.m4a\",model)\n",
    "print(f\"Similarity Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c178d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KASHF KAMAL\\AppData\\Local\\Temp\\ipykernel_16624\\3243090477.py:6: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio1, sr1 = librosa.load(file1, sr=16000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Similarity Score: 0.20\n"
     ]
    }
   ],
   "source": [
    "score = predict_similarity2(r\"dataurdu1/alphabets/alphabets/ص/Gali 79A 22.m4a\",r\"dataurdu1/alphabets/alphabets/ص/20_sawad.mp3\",model)\n",
    "print(f\"Similarity Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98620c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
